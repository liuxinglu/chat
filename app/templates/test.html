<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Microphone Audio Capture</title>
    <style>
        canvas {
            border: 1px solid black;
        }
    </style>
</head>
<body>
    <h1>Microphone Audio Capture</h1>
    <button id="startButton">Start Recording</button>
    <canvas id="canvas" width="800" height="200"></canvas>
    <script>
        const startButton = document.getElementById('startButton');
        const canvas = document.getElementById('canvas');
        const canvasCtx = canvas.getContext('2d');
        let audioContext;
        let mediaStreamSource;
        let analyser;
        let dataArray;
        let bufferLength;

        startButton.addEventListener('click', async () => {
            try {
                // Get audio context
                audioContext = new (window.AudioContext || window.webkitAudioContext)();

                // Request user media (microphone)
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

                // Create an AudioNode from the media stream
                mediaStreamSource = audioContext.createMediaStreamSource(stream);

                // Create an analyser node
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048;
                bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);

                // Connect the media stream source to the analyser
                mediaStreamSource.connect(analyser);

                // Draw the waveform
                draw();

                startButton.textContent = 'Stop Recording';
            } catch (err) {
                console.error('Error accessing microphone: ', err);
            }
        });

        startButton.addEventListener('click', () => {
            if (audioContext) {
                // Disconnect the audio context to stop processing
                audioContext.close().then(() => {
                    startButton.textContent = 'Start Recording';
                });
            }
        });

        function draw() {
            const drawVisual = requestAnimationFrame(drawVisual);

            analyser.getByteTimeDomainData(dataArray);

            canvasCtx.fillStyle = 'rgb(200, 200, 200)';
            canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

            canvasCtx.lineWidth = 2;
            canvasCtx.strokeStyle = 'rgb(0, 0, 0)';

            canvasCtx.beginPath();

            const sliceWidth = canvas.width * 1.0 / bufferLength;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                const v = dataArray[i] / 128.0;
                const y = v * canvas.height / 2;

                if (i === 0) {
                    canvasCtx.moveTo(x, y);
                } else {
                    canvasCtx.lineTo(x, y);
                }

                x += sliceWidth;
            }

            canvasCtx.lineTo(canvas.width, canvas.height / 2);
            canvasCtx.stroke();

            cancelAnimationFrame(drawVisual);
            drawVisual();
        }
    </script>
</body>
</html>